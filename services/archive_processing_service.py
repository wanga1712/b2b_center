"""
Ğ¡ĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ñ€Ñ…Ğ¸Ğ²Ğ¾Ğ² Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ°.
"""

from __future__ import annotations

from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Sequence, TYPE_CHECKING
import time
import re

from loguru import logger

from services.helpers.archive_cleanup import ArchiveCleanupManager

if TYPE_CHECKING:
    from services.document_search_service import DocumentSearchService


def format_number(value: str) -> str:
    """Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡Ğ¸ÑĞ»Ğ° Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€ÑĞ´Ğ¾Ğ²."""
    if not value:
        return value

    cleaned = value.strip().replace("\n", " ").replace("\r", " ")
    number_pattern = r"\d+(?:[.,]\d+)?"

    def _format(match: re.Match) -> str:
        num = match.group(0)
        separator = ""
        if "." in num:
            integer_part, decimal_part = num.split(".", 1)
            separator = "."
        elif "," in num:
            integer_part, decimal_part = num.split(",", 1)
            separator = ","
        else:
            integer_part, decimal_part = num, ""

        chunks = []
        for index, digit in enumerate(reversed(integer_part)):
            if index and index % 3 == 0:
                chunks.append(" ")
            chunks.append(digit)
        formatted_integer = "".join(reversed(chunks))
        return f"{formatted_integer}{separator}{decimal_part}" if decimal_part else formatted_integer

    return re.sub(number_pattern, _format, cleaned)


def find_archives_in_directory(directory: Path) -> Dict[str, List[Path]]:
    """ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ğ²Ñ‹ Ğ² Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ (Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ°Ğ¿ĞºĞ¸) Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¸Ğ¼ĞµĞ½Ğ¸."""
    archives: Dict[str, List[Path]] = defaultdict(list)
    archive_extensions = {".rar", ".zip", ".7z"}

    if not directory.exists():
        logger.error(f"Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚: {directory}")
        return archives

    for file_path in directory.rglob("*"):
        if not file_path.is_file():
            continue
        suffix = file_path.suffix.lower()
        if suffix not in archive_extensions:
            continue
        base_name = _extract_base_name(file_path.name)
        archives[base_name].append(file_path)

    for base_name in archives:
        archives[base_name].sort(key=lambda p: _extract_part_number(p.name))

    return archives


def _extract_base_name(filename: str) -> str:
    name_without_ext = Path(filename).stem
    patterns = [
        r"\.part\d+$",
        r"\.part\s*\d+$",
        r"[._-]\d+$",
    ]
    for pattern in patterns:
        match = re.search(pattern, name_without_ext, re.IGNORECASE)
        if match:
            return name_without_ext[: match.start()].strip("._-")
    return name_without_ext


def _extract_part_number(filename: str) -> int:
    name_without_ext = Path(filename).stem
    patterns = [
        r"\.part(\d+)$",
        r"\.part\s*(\d+)$",
        r"[._-](\d+)$",
    ]
    for pattern in patterns:
        match = re.search(pattern, name_without_ext, re.IGNORECASE)
        if match:
            return int(match.group(1))
    return 0


@dataclass
class ArchiveGroupResult:
    base_name: str
    workbook_paths: List[Path]
    matches: List[Dict[str, Any]]
    extract_dirs: List[Path]
    processing_time: float
    total_size: float


class ArchiveProcessingService:
    """ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ñ€Ñ…Ğ¸Ğ²Ğ¾Ğ²."""

    def __init__(
        self,
        document_service: "DocumentSearchService",
        cleanup_manager: Optional[ArchiveCleanupManager] = None,
    ):
        self.document_service = document_service
        self.cleanup_manager = cleanup_manager or ArchiveCleanupManager()

    def process_archive_group(
        self,
        base_name: str,
        archive_paths: Sequence[Path],
    ) -> ArchiveGroupResult:
        """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ñƒ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñƒ Ğ°Ñ€Ñ…Ğ¸Ğ²Ğ¾Ğ²."""
        start = time.time()
        result = self.document_service.debug_process_local_archives(
            [str(path) for path in archive_paths]
        )
        workbook_paths = [Path(p) for p in result.get("workbook_paths", [])]
        matches = result.get("matches", [])
        extract_dirs = [Path(p) for p in result.get("extract_dirs", [])]

        total_size = 0.0
        for workbook in workbook_paths:
            if workbook.exists():
                total_size += workbook.stat().st_size

        processing_time = time.time() - start

        try:
            self.cleanup_manager.cleanup(
                archive_paths,
                extract_dirs,
                matches,
            )
        except Exception as error:
            logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ°Ñ€Ñ…Ğ¸Ğ²Ğ° {base_name}: {error}")

        return ArchiveGroupResult(
            base_name=base_name,
            workbook_paths=workbook_paths,
            matches=matches,
            extract_dirs=extract_dirs,
            processing_time=processing_time,
            total_size=total_size,
        )

    @staticmethod
    def group_matches_by_score(matches: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."""
        groups = {"exact": [], "good": []}
        for match in matches:
            score = match.get("score", 0)
            if score >= 100:
                groups["exact"].append(match)
            elif score >= 85:
                groups["good"].append(match)
        return groups

    @staticmethod
    def build_display_chunks(match: Dict[str, Any], download_dir: Path) -> Dict[str, str]:
        """Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°."""
        file_info = ArchiveProcessingService._build_file_info(match, download_dir)
        summary_line = ArchiveProcessingService._build_summary_line(match)
        cell_text = ArchiveProcessingService._build_cell_text(match)

        return {
            "file_info": file_info,
            "summary": summary_line,
            "cell_text": cell_text,
        }
    
    @staticmethod
    def _build_file_info(match: Dict[str, Any], download_dir: Path) -> str:
        """Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğµ"""
        source_file = Path(match.get("source_file", ""))
        try:
            relative_file = source_file.relative_to(download_dir)
        except ValueError:
            relative_file = source_file
        
        return (
            f"ğŸ“„ {relative_file} | ğŸ“ Ğ›Ğ¸ÑÑ‚: {match.get('sheet_name')} "
            f"({match.get('cell_address')})"
        )
    
    @staticmethod
    def _build_summary_line(match: Dict[str, Any]) -> str:
        """Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ¾ĞºÑƒ Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹"""
        row_data = match.get("row_data") or {}
        field_configs = [
            ("ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾", "ğŸ“¦", "ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾"),
            ("ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ_ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹", "ğŸ’°", "Ğ¡Ñ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹"),
            ("Ğ¾Ğ±Ñ‰Ğ°Ñ_ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ", "ğŸ’µ", "ĞĞ±Ñ‰Ğ°Ñ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ"),
        ]
        
        chunks = []
        for field_key, icon, default_name in field_configs:
            if field_key in row_data:
                info = row_data[field_key]
                chunks.append(
                    f"{icon} {info.get('name', default_name)} ({info.get('column', '?')}): "
                    f"{format_number(str(info.get('value')))}"
                )
        
        return " | ".join(chunks) if chunks else ""
    
    @staticmethod
    def _build_cell_text(match: Dict[str, Any]) -> str:
        """Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ ÑÑ‡ĞµĞ¹ĞºĞ¸"""
        cell_text = match.get("matched_display_text") or match.get("matched_text") or ""
        cleaned_text = " ".join(str(cell_text).split())
        if len(cleaned_text) > 200:
            cleaned_text = f"{cleaned_text[:200]}..."
        return f"ğŸ“ Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ°: {cleaned_text}"

